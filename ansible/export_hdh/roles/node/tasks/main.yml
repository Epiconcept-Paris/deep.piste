---
- name: Transform folder is not present
  file:
    state: absent
    path: "{{ transform_path }}"
- name: folders exists on all nodes
  file:
    path:
    - "{{ dp_home  }}"
    - "{{ dp_code  }}"
    - "{{ dpiste_git  }}"
    - "{{ kskit_git  }}"
    state: directory
    mode: 0700
    owner: "{{ ssh_user }}"
    group: "{{ ssh_user }}"
  tags: [export]
- name: deep.piste github libraries are updated
  git:
    repo: "{{ item.repo }}"
    dest: "{{ dp_code }}/{{ item.name }}" 
    version: "{{ item.version }}"
  with_items:
  - {name: "deep.piste", repo: "https://github.com/Epiconcept-Paris/deep.piste/", version: "main" }
  - {name: "kskit", repo: "https://github.com/Epiconcept-Paris/kskit", version: "main" }
  environment:
    TMPDIR: "{{ tmp_dir }}"
    HTTPS_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
    HTTP_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
  become_user: "{{ ssh_user }}"
  tags: [export]
- name: ensuring gpg agent is not currently running
  shell: |
          pkill -u {{ ssh_user  }} gpg-agent 
  become_user: "{{ ssh_user }}"
  tags: [export]
- name: creating deep.piste virtual envirobment on nodes
  shell: |
          export PYTHON={{ python_bin }}
          export PATH={{ python_path }}:$PATH
          make init
  args:
    chdir: "{{ dpiste_git  }}"
    creates: "{{ dpiste_git }}/env"
  become_user: "{{ ssh_user }}"
  tags: [export]
- name: installing deep.piste on nodes  
  shell: |
          export PATH={{ python_path }}:$PATH
          make install
  args:
    chdir: "{{ dpiste_git  }}"
    creates: "{{ dpiste_git }}/env/lib/python3.6/site-packages/pyzbar"
  become_user: "{{ ssh_user }}"
  tags: [export]
- name: copying private key from local to hosts
  copy:
    src: "{{ ssh_source_key }}"
    dest: "{{ ssh_tmp_key_remote }}"
    force: true
    owner: francisco
    group: francisco
    mode: 0600
- name: copying public key from local to hosts
  copy:
    src: "{{ ssh_source_key_pub }}"
    dest: "{{ ssh_tmp_key_remote_pub }}"
    force: true
    owner: francisco
    group: francisco
    mode: 0644
- name: private key has no passphrase 
  command: |
          ssh-keygen -p -P '{{ ssh_passphrase }}' -N '' -f {{ ssh_tmp_key_remote  }}
  become_user: "{{ ssh_user }}"
  tags: [export]



#
#- name: get cpu
#  tags: [configure_map_reduce, upgrade, scale]
#  command: "nproc --all"
#  register: node_cpu
#  changed_when: false
#- name: get host available memory
#  tags: [upgrade, configure_yarn, scale]
#  shell: "cat /proc/meminfo | grep MemTotal | awk '{print int($2/1024)}'"
#  register: out
#  changed_when: false
#- name: get available memory
#  set_fact:
#    available_memory_mb: "{{ out.stdout }}"
#  tags: [upgrade, configure_yarn, scale]
#- name: get available memory for yarn node manager
#  set_fact:
#    yarn_nodemanager_memory_mb: "{{ (available_memory_mb | int - 800 )| int }}"
#  tags: [upgrade, configure_yarn, scale]
#- name: crypted disk is running
#  tags: [check_hosts, upgrade]
#  shell: "if mountpoint -q /{{ crypted_disk }}; then echo 'MOUNTED'; else echo 'NOT-MOUNTED'; fi"
#  register: mount_output
#  changed_when: false
#- name: start crypted disk
#  tags: [check_hosts, upgrade]
#  shell: |
#    echo -n "{{ decrypt_disk_pass }}" | cryptsetup luksOpen --key-file=- /dev/{{ inventory_hostname.split('.')[0] }}-crypt-1/{{ crypted_disk  }} {{ crypted_disk  }}
#  args:
#    executable: /bin/bash
#  register: out
#  changed_when: out.stdout=='' and out.stderr==''
#  failed_when: out.stderr != '' and not out.stderr.endswith('already exists.')
#  when: mount_output.stdout == 'NOT-MOUNTED'
#- name: mount crypted disk 
#  tags: [check_hosts, upgrade]
#  shell: "mount /{{ crypted_disk  }}"
#  register: out
#  changed_when: true 
#  when: mount_output.stdout == 'NOT-MOUNTED'
#- name: installation folder
#  file:
#    path: "{{ hadoop_install }}"
#    state: directory
#    mode: 0751
#    owner: hadoop
#    group: hadoop
#  tags: upgrade
#- name: custom tmp folder present
#  file:
#    path: "{{ tmp_dir }}"
#    state: directory
#    mode: u=rwx,g=rwx,o=rwx,g+s
#    owner: root
#    group: root
#  tags: [upgrade, tmp]
#- name: custom libs folder exists
#  file:
#    path: "{{ custom_libs }}"
#    state: directory
#    mode: 0771
#    owner: hadoop
#    group: hadoop
#  tags: [upgrade, packages, python]
#- name: backports are activated
#  apt_repository:
#    repo: "deb http://ftp.fr.debian.org/debian {{ ansible_distribution_release }}-backports main"
#  tags: upgrade
#- name: install backports packages
#  tags: [packages, upgrade]
#  apt:
#    name: "{{ item }}"
#    default_release: "{{ ansible_distribution_release }}-backports"
#    update_cache: true
#    state: latest
#  with_items: ["{{ java_package }}", "{{ java_package_spark }}", maven, r-base  ]
#  notify: [set_java_default, set_java_for_R]
#- name: set java home
#  tags: [java, upgrade]
#  lineinfile:
#    path: "/etc/environment" 
#    regexp: '^JAVA_HOME='
#    line: "JAVA_HOME={{ java_home }}"
#  notify: set_java_default
#- name: Getting key for installing llvm
#  tags: [packages, upgrade, python]
#  apt_key:
#    data: "{{ lookup('url', 'https://apt.llvm.org/llvm-snapshot.gpg.key', split_lines=False) }}" 
#    state: present
#  environment:
#    HTTPS_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#    HTTP_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#- name: Ensuring a fresh version llvm is present
#  tags: [packages, upgrade, python]
#  apt_repository:
#    repo: "deb http://apt.llvm.org/{{ ansible_distribution_release }}/ llvm-toolchain-{{ ansible_distribution_release }}-6.0 main"
#    state: present
#- name: get node version
#  tags: [packages, upgrade, test]
#  shell: "if [ -f /usr/bin/nodejs  ]; then if [ `/usr/bin/nodejs --version` == 'v10.12.0' ]; then echo 'OK'; else echo 'UPDATE'; fi; else echo 'UPDATE'; fi"
#  register: updatenode
#  changed_when: false
#- name: install node if necessary
#  tags: [packages, upgrade, test]
#  shell: | 
#    export http_proxy=http://{{ proxy_host }}:{{ proxy_port }}
#    export https_proxy=http://{{ proxy_host }}:{{ proxy_port }}
#    curl -sL https://deb.nodesource.com/setup_10.x | bash -
#    apt-get install -y nodejs
#  register: updatenode
#  changed_when: true
#  when: updatenode.stdout == 'UPDATE'
#  notify: [ set_npm_proxy ]
#- name: installing packages
#  tags: [packages, upgrade]
#  apt:
#    name: "{{ item }}"
#    update_cache: true
#    state: latest
#  with_items: [build-essential, g++, autoconf, automake, libtool, zlib1g-dev, pkg-config, libssl-dev, xmlstarlet, git, libfontconfig ,pandoc, proj-data, locales, curl, jq ]
#  notify: [ set_cran_mirror ]
#- name: installing backportes packages
#  tags: [packages, upgrade, python, blas]
#  apt:
#    name: "{{ item }}"
#    default_release: "{{ ansible_distribution_release }}-backports"
#    update_cache: true
#    state: latest
#  with_items: [python-setuptools, libgdal-dev, libgeos-dev, libproj-dev, cmake, llvm-6.0, libbz2-dev, libreadline-gplv2-dev, libsqlite3-dev, libncurses5-dev, libncursesw5-dev, xz-utils,tk-dev, libgdbm-dev, libc6-dev, libssl-dev, libatlas3-base, libopenblas-base ] #libgfortran-6-dev, python3, python3-pip, python3-numpy, python3-scipy]
#  notify: [ set_npm_proxy, set_cran_mirror ]
#- name: download python
#  tags: [python, upgrade]
#  command: "wget https://www.python.org/ftp/python/{{ python_ver  }}/Python-{{ python_ver  }}.tgz"
#  args:
#    chdir: "{{ custom_libs }}"
#    creates: "{{ custom_libs }}/Python-{{ python_ver }}.tgz"
#  become_user: spark
#- name: uncompress python
#  tags: [python, upgrade]
#  unarchive:
#    src: "{{ custom_libs }}/Python-{{ python_ver }}.tgz"
#    dest: "{{ custom_libs }}"
#    remote_src: true 
#  become_user: spark
#- name: building python
#  tags: [packages, upgrade, python]
#  shell: |
#   ./configure --enable-optimizations --with-ensurepip=install --prefix {{ custom_libs }}/python-{{ python_ver }}-lib --cache-file {{ tmp_dir }}/python-build
#   make -j4
#   make install
#  args:
#    chdir: "{{ custom_libs }}/Python-{{ python_ver }}"
#    creates: "{{ custom_libs }}/python-{{ python_ver }}-lib"
#  become_user: spark
#  environment:
#    TMPDIR: "{{ tmp_dir }}"
#- name: create link to current python version
#  tags: [python, upgrade]
#  file:
#    src: "{{ custom_libs }}/python-{{ python_ver }}-lib" 
#    dest: "{{ python_home }}"
#    owner: spark
#    group: hadoop
#    mode: 0751
#    state: link
#    #force: true
#- name: upgrade pip
#  tags: [packages, upgrade, python]
#  pip:
#    name: pip
#    state: latest
#    executable: "{{ pip }}"
#  environment:
#    TMPDIR: "{{ tmp_dir }}"
#    HTTPS_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#    HTTP_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#    LLVM_CONFIG: "/usr/lib/llvm-6.0/bin/llvm-config"
#- name: update requirement files
#  tags: [packages, upgrade, python]
#  copy:
#    dest: "{{ custom_libs }}/pip_requirements"
#    content: |
#      git+https://github.com/gutfeeling/word_forms.git#egg=word_forms
#      pandas
#      scikit-learn
#      nltk
#      contractions
#      inflect
#      pprint
#      gensim
#      datetime
#      python-dateutil
#      pyarrow
#      imbalanced-learn
#- name: install python packages
#  tags: [packages, upgrade, python]
#  pip:
#    requirements: "{{ custom_libs }}/pip_requirements"
#    executable: "{{ pip }}"
#  environment:
#    TMPDIR: "{{ tmp_dir }}"
#    HTTPS_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#    HTTP_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#    LLVM_CONFIG: "/usr/lib/llvm-6.0/bin/llvm-config"
#- name: install R packages for Spark
#  tags: [packages, upgrade, r_packages]
#  shell: |
#    export http_proxy=http://{{ proxy_host }}:{{ proxy_port }}
#    export https_proxy=http://{{ proxy_host }}:{{ proxy_port }}
#    export ftp_proxy={{ proxy_host }}:{{ proxy_port }}
#    R -e "list.of.packages <- c(\"{{ spark_r_packages_url_name | join('\", \"') }}\");list.of.urls <- c(\"{{ spark_r_packages_url | join('\", \"') }}\");map <- new.env(hash=T, parent=emptyenv());for(i in 1:length(list.of.packages)) {map[[list.of.packages[i]]]<-list.of.urls[i]};new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]; if(length(new.packages)) install.packages(lapply(new.packages, function(x) map[[x]]), repos=NULL, type='source')"
#    R -e "list.of.packages <- c(\"{{ spark_r_packages | join('\", \"') }}\"); new.packages <- list.of.packages[!(list.of.packages %in% installed.packages()[,\"Package\"])]; if(length(new.packages)) install.packages(new.packages, repos=c('https://cran.univ-paris1.fr','https://ftp.igh.cnrs.fr/pub/CRAN/','http://cran.irsn.fr/'))"
#  register: out
#  failed_when: (out.stderr | regex_search('(^|[\\n])(ERROR|WARNING)')|string) in 'ERROR,WARNING,\nERROR,\nWARNING'
#  changed_when: out.stderr|length > 0
#  notify: restart zeppelin
#- name: downloading github libraries
#  tags: [packages, upgrade, python]
#  git:
#    repo: "{{ item.repo }}"
#    dest: "{{ custom_libs }}/{{ item.name }}" 
#    version: "{{ item.version }}"
#  with_items:
#  - {name: "WDDS", repo: "https://github.com/WDDS/Tweet-Classification-Diabetes-Distress.git", version: "devAA" }
#  environment:
#    TMPDIR: "{{ tmp_dir }}"
#    HTTPS_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#    HTTP_PROXY: "http://{{ proxy_host }}:{{ proxy_port }}"
#  become_user: hadoop
#- name: download google protocol buffer
#  command: "wget -nc https://github.com/google/protobuf/releases/download/v{{ protobuf_ver }}/protobuf-{{ protobuf_ver }}.tar.gz"
#  args:
#    chdir: "{{ hadoop_install }}"
#    creates: "{{ hadoop_install }}/protobuf-{{ protobuf_ver }}.tar.gz"
#  tags: [protobuf, upgrade]
#- name: uncompress protocol buffers 
#  unarchive:
#    src: "{{ hadoop_install }}/protobuf-{{ protobuf_ver }}.tar.gz"
#    dest: "/usr/local/src"
#    remote_src: true 
#    creates: "/usr/local/src/protobuf-{{ protobuf_ver }}" 
#  tags: [protobuf, upgrade]
#- name: make/install protocol buffers
#  command: "{{ item }}"
#  args:
#    chdir: "/usr/local/src/protobuf-{{ protobuf_ver }}"
#    creates: /usr/bin/protoc
#  with_items:
#  - ./autogen.sh
#  - ./configure --prefix=/usr
#  - make
#  - make install
#  tags: [protobuf, upgrade]
#- name: install protocols buffers for java
#  command: "mvn install -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}"
#  args:
#    chdir: "/usr/local/src/protobuf-{{ protobuf_ver }}/java"
#    creates: "/usr/local/src/protobuf-{{ protobuf_ver }}/java/target/protobuf-java-{{ protobuf_ver }}.jar"
#  tags: [protobuf, upgrade]
#- meta: flush_handlers
#- name: download hadoop if does not exists
#  tags: [install_hadoop, upgrade]
#  command: "wget {{ apache_mirror }}/pub/apache/hadoop/common/hadoop-{{ hadoop_ver }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
#  args:
#    chdir: "{{ hadoop_install }}"
#    creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
#- name: uncompress hadoop
#  tags: [install_hadoop, upgrade]
#  unarchive:
#    src: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src.tar.gz"
#    dest: "{{ hadoop_install }}"
#    remote_src: true 
#- name: Compile hadoop
#  tags: [install_hadoop, upgrade]
#  become_user: hadoop
#  shell: |
#          export JAVA_HOME={{ java_home }}
#          mvn package -Pdist,native -DskipTests -Dtar -Dhttp.proxyHost={{ proxy_host }} -Dhttp.proxyPort={{ proxy_port }} -Dhttps.proxyHost={{ proxy_host }} -Dhttps.proxyPort={{ proxy_port }}
#  args:
#    chdir: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src"
#    #creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-dist-{{ hadoop_ver }}.jar"
#    creates: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-{{ hadoop_ver }}/share/hadoop/hdfs/hadoop-hdfs-{{ hadoop_ver }}.jar"
#- name: copy compiled version (no overwrite)
#  tags: [install_hadoop, upgrade]
#  shell: "cp -R -v -n {{ hadoop_install }}/hadoop-{{ hadoop_ver }}-src/hadoop-dist/target/hadoop-{{ hadoop_ver }} {{ hadoop_install }}/ | wc -l"
#  register: out
#  changed_when: out.stdout!='0'
#- name: create link to current version
#  tags: [install_hadoop, upgrade]
#  file:
#    src: "{{ hadoop_install }}/hadoop-{{ hadoop_ver }}" 
#    dest: "{{ hadoop_home }}"
#    owner: hadoop
#    group: hadoop
#    mode: 0751
#    state: link
#    #force: true
#- name: set hadoop home
#  tags: [configure_hadoop, upgrade]
#  lineinfile:
#    path: "/etc/environment" 
#    regexp: '^HADOOP_PREFIX='
#    line: "HADOOP_PREFIX={{ hadoop_home  }}"
#- name: ensure conf directory exists
#  tags: [configure_hadoop, upgrade]
#  file:
#    path: "{{ item }}"
#    state: directory
#    mode: 0750
#    owner: hadoop
#    group: hadoop
#  with_items:
#  - "{{ hadoop_conf_dir }}"
#- name: ensure log et pid folder exists
#  tags: [configure_hadoop, upgrade]
#  file:
#    path: "{{ item }}"
#    state: directory
#    mode: 0770
#    owner: hadoop
#    group: hadoop
#  with_items:
#  - "{{ hadoop_log_dir }}"
#  - "{{ hadoop_pid_dir }}"
#- name: copy conf files (no overwrite)
#  tags: [configure_hadoop, upgrade]
#  shell: "cp -R -v -n {{ hadoop_home}}/etc/hadoop/* {{ hadoop_conf_dir }} | wc -l"
#  register: out
#  changed_when: out.stdout!='0'
#- name: update hadoop-env.sh
#  tags: [configure_hadoop, upgrade]
#  lineinfile:
#    path: "{{ hadoop_conf_dir }}/hadoop-env.sh" 
#    regexp: "^export {{ item.var  }}"
#    line: "export {{ item.var }}=\"{{ item.value }}\""
#  with_items:
#  - {var: "JAVA_HOME", value: "{{ java_home  }}" }
#  - {var: "HADOOP_LOG_DIR", value: "{{ hadoop_log_dir }}" }
#  - {var: "HADOOP_CONF_DIR", value: "{{ hadoop_conf_dir  }}" }
#  - {var: "HADOOP_PID_DIR", value: "{{ hadoop_pid_dir  }}" }
#  - {var: "HADOOP_NAMENODE_OPTS", value: "-Xmx{{ hdfs_namenode_daemon_memory_mb }}m" }
#  - {var: "HADOOP_DATANODE_OPTS", value: "-Xmx{{ hdfs_datanode_daemon_memory_mb }}m" }
#  - {var: "YARN_RESOURCEMANAGER_OPTS", value: "-Xmx{{ yarn_resourcemanager_daemon_memory_mb }}m" }
#  - {var: "YARN_NODEMANAGER_OPTS", value: "-Xmx{{ yarn_nodemanager_daemon_memory_mb }}m" }
#  - {var: "YARN_PROXYSERVER_OPTS", value: "-Xmx{{ yarn_webproxy_daemon_memory_mb  }}m" }
#  - {var: "HADOOP_JOB_HISTORYSERVER_OPTS", value: "-Xmx{{ mapreduce_jobhistory_memory_mb }}m" }
#  notify: [restart hdfs, restart yarn]
#- name: copy script to update hadoop xml files
#  tags: [configure_hadoop, upgrade, copy_xml, scale, configure_yarn, configure_map_reduce, upgrade, install_zeppelin, install_spark, configure_spark]
#  copy:
#    src: "./xmlpresent.sh"
#    dest: "/tmp/xmlpresent.sh"
#    owner: hadoop
#    group: hadoop
#    mode: 0770
#- name: update core-site.xml
#  tags: [configure_hadoop, upgrade]
#  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/core-site.xml"
#  register: out
#  changed_when: not out.stdout.startswith('NO-CHANGE')
#  with_items:
#  - {var: "fs.defaultFS", value: "hdfs://{{ groups['namenode'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ hdfs_port }}" }
#  - {var: "io.file.buffer.size", value: "{{ hdfs_file_buffer_bytes }}" }
#  - {var: "hadoop.proxyuser.root.hosts", value: "{{ groups['hdfs_edge'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
#  - {var: "hadoop.proxyuser.root.groups", value: "*" }
#  - {var: "hadoop.proxyuser.Spark.hosts", value: "{{ groups['hdfs_edge'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
#  - {var: "hadoop.proxyuser.spark.groups", value: "*" }
#  notify: [restart hdfs, restart yarn]
#- name: yarn log folder exist 
#  tags: [configure_yarn, upgrade]
#  file:  
#    path: "{{ yarn_log_dir }}"
#    state: directory
#    mode: 0751
#    owner: yarn
#    group: hadoop
#- name: setting necessary environment variable
#  tags: [configure_yarn, upgrade]
#  lineinfile:
#    path: "{{ hadoop_conf_dir }}/yarn-env.sh" 
#    regexp: "^export {{ item.var  }}"
#    line: "export {{ item.var }}={{ item.value }}"
#  with_items:
#  - {var: "YARN_LOG_DIR", value: "{{ yarn_log_dir }}"} 
#  - {var: "YARN_PID_DIR", value: "{{ hadoop_pid_dir  }}"}
#  notify: restart yarn
#- name: update yarn-site.xml for resouerce manager
#  tags: [configure_yarn, upgrade, scale]
#  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/yarn-site.xml"
#  register: out
#  changed_when: not out.stdout.startswith('NO-CHANGE')
#  with_items:
#  - {var: "yarn.acl.enable", value: "{{ yarn_acl  }}" }
#  - {var: "yarn.admin.acl", value: "{{ yarn_admin_acl  }}" }
#  - {var: "yarn.log-aggregation-enable", value: "{{ yarn_log_aggregation }}" }
#  - {var: "yarn.resourcemanager.address", value: "{{ groups['resourcemanager'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ yarn_resourcemanager_port }}" }
#  - {var: "yarn.resourcemanager.scheduler.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_resourcescheduler_port  }}" }
#  - {var: "yarn.resourcemanager.resource-tracker.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_resourcetracker_port }}" }
#  - {var: "yarn.resourcemanager.admin.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_admin_port  }}" }
#  - {var: "yarn.resourcemanager.webapp.address", value: "{{ groups['resourcemanager'][0].split('.')[0]  }}-{{ hadoop_cluster_name }}:{{ yarn_webapp_port }}" }
#  - {var: "yarn.resourcemanager.scheduler.class", value: "{{ yarn_scheduler }}" }
#  - {var: "yarn.resourcemanager.recovery.enabled", value: "{{ yarn_resourcemanager_recovery }}" }
#  - {var: "yarn.resourcemanager.store.class", value: "{{ yarn_resourcemanager_store_class }}" }
#  - {var: "yarn.resourcemanager.fs.state-store.uri", value: "{{ yarn_resourcemanager_store_uri }}" }
#  - {var: "yarn.scheduler.minimum-allocation-mb", value: "{{ yarn_container_min_mb }}" }
#  - {var: "yarn.scheduler.maximum-allocation-mb", value: "{{ yarn_container_max_mb }}" }
#  - {var: "yarn.scheduler.maximum-allocation-vcores", value: "{{ yarn_container_max_cores }}" }
#  - {var: "yarn.resourcemanager.bind-host", value: "{{ groups['resourcemanager'][0].split('.')[0] }}-{{ hadoop_cluster_name }}" }
#  - {var: "yarn.web-proxy.address", value: "{{ groups['yarn_edge'][0].split('.')[0] }}-{{ hadoop_cluster_name }}:{{ yarn_webproxy_port }}" }
#  notify: restart yarn
#- name: update capacity-scheduler.xml for resouerce manager
#  tags: [configure_yarn, upgrade]
#  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/capacity-scheduler.xml"
#  register: out
#  changed_when: not out.stdout.startswith('NO-CHANGE')
#  with_items:
#  - {var: "yarn.scheduler.capacity.resource-calculator", value: "{{ yarn_scheduler_capacity_calculator }}" }
#  notify: restart yarn
#- name: update yarn-site.xml on node managers
#  tags: [configure_yarn, upgrade, scale]
#  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/yarn-site.xml"
#  register: out
#  changed_when: not out.stdout.startswith('NO-CHANGE')
#  with_items:
#  - {var: "yarn.nodemanager.local-dirs", value: "{{ yarn_nodemanager_local_dirs  }}" }
#  - {var: "yarn.nodemanager.log-dirs", value: "{{ yarn_log_dir }}" }
#  - {var: "yarn.nodemanager.log.retain-seconds", value: "{{ yarn_nodemanager_log_seconds }}" }
#  - {var: "yarn.nodemanager.remote-app-log-dir", value: "{{ yarn_nodemanager_remote_log_dir }}" }
#  - {var: "yarn.nodemanager.remote-app-log-dir-suffix", value: "{{ yarn_nodemanager_remote_log_dir_suffix  }}" }
#  - {var: "yarn.nodemanager.resource.detect-hardware-capabilities", value: "{{ yarn_nodemanager_detect_hardware }}" }
#  - {var: "yarn.nodemanager.resource.memory-mb", value: "{{ yarn_nodemanager_memory_mb  }}" }
#  - {var: "yarn.nodemanager.resource.system-reserved-memory-mb", value: "{{ yarn_nodemanager_system_reserved_memory_mb  }}" }
#  - {var: "yarn.nodemanager.resource.cpu-vcores", value: "{{ yarn_nodemanager_resource_vcores  }}" }
#  - {var: "yarn.nodemanager.resource.count-logical-processors-as-cores", value: "{{ yarn_nodemanager_logical_procs_as_cores }}" }
#  - {var: "yarn.nodemanager.resource.pcores-vcores-multiplier", value: "{{ yarn_nodemanager_resource_pcores_multiplier }}" }
#  - {var: "yarn.nodemanager.vmem-check-enabled", value: "{{ yarn_nodemanager_check_vmem }}" }
#  - {var: "yarn.nodemanager.pmem-check-enabled", value: "{{ yarn_nodemanager_check_pmem }}" }
#  - {var: "yarn.nodemanager.vmem-pmem-ratio", value: "{{ yarn_nodemanager_vmem_pmem_ratio  }}" }
#  - {var: "yarn.nodemanager.hostname", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}" }
#  - {var: "yarn.nodemanager.bind-host", value: "{{ inventory_hostname.split('.')[0] }}-{{ hadoop_cluster_name }}" }
#  - {var: "yarn.nodemanager.aux-services", value: "spark_shuffle" }
#  - {var: "yarn.nodemanager.aux-services.spark_shuffle.class", value: "org.apache.spark.network.yarn.YarnShuffleService" }
#  notify: restart yarn
#- name: copy conf file xml files if template not exists use aready existing
#  tags: [configure_map_reduce, upgrade]
#  copy:
#    src: "{{ hadoop_conf_dir }}/mapred-site.xml"
#    dest: "{{ hadoop_conf_dir }}/mapred-site.xml.template"
#    remote_src: True
#    force: false
#    owner: yarn
#    group: hadoop
#    mode: 0770
#- name: copy conf file xml files
#  tags: [configure_map_reduce, upgrade]
#  copy:
#    src: "{{ hadoop_conf_dir }}/mapred-site.xml.template"
#    dest: "{{ hadoop_conf_dir }}/mapredsite.xml"
#    remote_src: True
#    force: false
#    owner: yarn
#    group: hadoop
#    mode: 0770
#- name: update mapred-site.xml on nodes
#  tags: [configure_map_reduce, upgrade, scale]
#  command: "/tmp/xmlpresent.sh --container-xpath \"/configuration\" --node \"property\" --property-node name --property-text \"{{ item.var  }}\" --value-node value --value \"{{ item.value  }}\" --file {{ hadoop_conf_dir }}/mapred-site.xml"
#  register: out
#  changed_when: not out.stdout.startswith('NO-CHANGE')
#  with_items:
#  - {var: "mapreduce.framework.name", value: "{{ mapreduce_framework_name }}" }
#  - {var: "mapreduce.map.memory.mb", value: "{{ ((1.5*available_memory_mb|int) / (node_cpu.stdout|int))|int }}" }
#  - {var: "mapreduce.map.java.opts", value: "-Xmx{{ (available_memory_mb|int / node_cpu.stdout|int)|int }}M" }
#  - {var: "mapreduce.reduce.memory.mb", value: "{{ ((2*available_memory_mb|int) / (node_cpu.stdout|int))|int }}" }
#  - {var: "mapreduce.reduce.java.opts", value: "-Xmx{{ (2*available_memory_mb|int / node_cpu.stdout|int)|int }}M" }
#  - {var: "mapreduce.task.io.sort.mb", value: "{{ ((0.5*available_memory_mb|int) / (node_cpu.stdout|int))|int }}" }
#  - {var: "mapreduce.task.io.sort.factor", value: "{{ mapreduce_task_io_sort_factor  }}" }
#  - {var: "mapreduce.reduce.shuffle.parallelcopies", value: "{{ mapreduce_reduce_shuffle_parallelcopies  }}" }
#  - {var: "mapreduce.jobhistory.address", value: "{{ groups['resourcemanager'][0].split('.')[0]}}:{{mapreduce_jobhistory_port  }}" }
#  - {var: "mapreduce.jobhistory.webapp.address", value: "{{ groups['resourcemanager'][0].split('.')[0]}}:{{ mapreduce_jobhistory_webapp_port }}" }
#  - {var: "mapreduce.jobhistory.intermediate-done-dir", value: "{{ mapreduce_jobhistory_intermediate_done_dir  }}" }
#  - {var: "mapreduce.jobhistory.done-dir", value: "{{ mapreduce_jobhistory_done_dir  }}" }
